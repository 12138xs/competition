# CLIP

‌CLIP（Contrastive Language-Image Pre-training）模型是由OpenAI在2021年提出的一种多模态预训练模型，旨在通过对比学习的方式将图像和文本嵌入到同一个语义空间中，从而理解它们之间的语义关系‌。

### 基本原理

CLIP模型的核心思想是通过最大化图像表示与其相应文本描述之间的一致性，来预训练一个能够同时理解图像和文本的模型。它采用双流架构，分别处理图像和文本数据，并通过对比学习将它们嵌入到同一个向量空间中，实现跨模态的信息交互与融合‌。

### 模型架构

CLIP模型由两个主要部分组成：图像编码器和文本编码器。图像编码器可以将图像转换为特征向量，通常使用卷积神经网络（如ResNet）或Transformer模型（如ViT）；文本编码器则将文本转换为特征向量，通常使用Transformer模型。这两个编码器通过共享一个向量空间来实现跨模态的信息交互与融合‌。

### 比赛规则

在精度保持不变的情况下，进行性能比拼，单token推理时间短者胜出。

精度保持不变：无法达到官方提供baseline/或模型精度降低的成绩无效，官方提供精度测试的UT。

单token推理时间：测试验证1000个token推理的平均时间（不包含prefill和decode的首token）。

### 修改内容

将慢的算子替换为mint中快的算子。

优化代码的运行逻辑，减少cpu和npu之间转换的时间。

尝试过flash_attention和静态图改造。

通过精度测试，其性能测试平均用时为0.034767802349312274s。