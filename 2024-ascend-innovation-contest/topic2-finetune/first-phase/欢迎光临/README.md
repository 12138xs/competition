### 一、微调算法介绍

使用了低秩自适应(Low-Rank Adaptation, LoRA)，它冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到Transformer架构的每一层，从而大大减少了下游任务的可训练参数的数量。与经过Adam微调的GPT-3 175B相比，LoRA可以将可训练参数的数量减少10,000倍，GPU内存需求减少3倍。

替换图1中的矩阵A和B来冻结共享模型并有效地切换任务

##### 出发点

模型是过参数化的，它们有更小的内在维度，模型主要依赖于这个低的内在维度（low intrinsic dimension）去做任务适配

具体的，在下游任务中微调一个预训练语言模型（如 GPT-3），则需要更新预训练模型参数，公式表示如下：

 $$ (1)𝑊0+Δ𝑊 $$

𝑊0 是预训练模型初始化的参数， Δ𝑊 就是需要更新的参数。如果是全参数微调，则它的参数量 =𝑊0 （如果是 GPT-3，则 Δ𝑊≈175B ）。从这可以看出要全参数微调大语言模型，代价是非常高的。

而对于 LORA 来说，只需要微调 Δ𝑊 。

 $$ (2)𝑊0+Δ𝑊=W0+BA $$

LORA 的这种思想有点类似于残差连接，同时使用这个旁路的更新来模拟 Full Fine-Tuning的过程。

在推理过程中，LoRA 也几乎未引入额外的 Inference Latency，只需要计算 𝑊=𝑊0+Δ𝑊 即可。

LoRA 与 Transformer 的结合也很简单，仅在 QKV Attention 的计算中增加一个旁路。

### 二、超参配置介绍说明

使用的默认超参数配置

### 三、微调后的权重文件链接

```
https://raceobs.obs.cn-southwest-2.myhuaweicloud.com/checkpoint_0.ckpt
```

### 四、运行环境说明

无额外配置

### 五、模型微调后原有能力评估得分

F1 score: 68.56560360764708，Em score: 51.76584421867441, total count: 2067


### 六、推理结果

增加了

```
    min_new_tokens: 1
```

