## 本作品使用的推理优化算法介绍
1.	Pd分离
 
将预填充（Prefill）和解码（Decoding）阶段分离，以优化计算效率和内存使用。这个技术特别在推理过程中有效，通过分别处理和优化这两个阶段，提升了模型生成长文本的性能。

预填充阶段涉及模型输入序列的预处理和缓存。通常包括将输入文本转换为内部表示，并计算初始的注意力键值对（KV-Pairs），以便后续生成步骤使用。

解码阶段是模型实际生成文本的过程。每一步生成一个新词，并使用先前生成的词和预填充阶段的缓存结果进行计算。

预填充阶段的缓存结果可以重复使用，避免了在解码过程中不断重新计算大规模的注意力矩阵。

此外，预填充阶段由于输入序列更长，其显存占用要大于解码阶段，可以减小一些预填充阶段的batch size。

2. Continues batching
连续批处理，当一个batch中较短的序列生成结束后可以接入一个新的请求。

3. 模型量化：采用fp16精度
   
4. 调整batch size，尽量提高显卡利用率（在所需计算量相同的情况下，显卡利用率越高，推理速度越快）


## 超参配置介绍

对于llama本身的超参，我们未作改动；对于llm-serving的超参，我们设置其perfill_batch_size为16，decode_batch_size为64，model_dtype为DataType.FLOAT16。其余超参我们未作改动，整体见：配置文件.yaml

对于performance_serving中test_serving_performance.py的超参，我们设置X=5,T=300。

## 优化后的推理总时长
总时长为823.84s。


## 运行环境说明
与文档中相同，未作改动。

