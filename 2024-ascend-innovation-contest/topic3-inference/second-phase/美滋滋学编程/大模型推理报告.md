# 推理调优作品报告

## 业界推理优化算法调研

#### flash attention

FlashAttention旨在**加速**注意力计算并**减少内存占用**。FlashAttention利用底层硬件的内存层次知识，例如GPU的内存层次结构，来提高计算速度和减少内存访问开销。 FlashAttention的核心原理是通过将输入**分块**并在每个块上执行注意力操作，从而减少对高带宽内存（HBM）的读写操作。具体而言，FlashAttention使用平铺和重计算等经典技术，将输入块从HBM加载到SRAM（快速缓存），在SRAM上执行注意力操作，并将结果更新回HBM。FlashAttention减少了内存读写量，从而实现了**2-4倍**的时钟时间加速。

#### paged attention

PagedAttention 的提出是为了解决大模型推理中 KV Cache 带来的显存空间利用率低的问题，该问题的主要原因在于现有的推理系统将 KV Cache 存储在连续的显存空间中，导致：

1. 内部碎片和外部碎片：由于 KV Cache 占用的显存大小随着 seq_len 动态变化，而对于不同的请求输入我们无法预先确定模型的输出序列长度，所以对于每个请求都需要预留 max_seq_len 对应的显存大小给 KV Cache。而在推理过程中所需要的 KV Cache 大小可能比预留的大小要小得多，但预留的这部分显存在请求的整个生命周期都被保留，未被使用的部分无法被其他请求利用，导致内部碎片严重。另一方面，外部内存碎片也可能很严重，因为每个请求的 max_seq_len 可能不同。
2. 无法进行内存共享：LLM 服务通常使用先进的解码算法，例如 parallel sampling 和 beam search，这些解码算法会为每个请求产生多个输出。在这些场景中，单个请求由多个序列（sequence）组成，这些序列具有公共前缀，它们可以共享 kv cache。然而，在现有系统中，内存共享是不可能的，因为每个序列的 kv cache 存储在单独的连续空间中，无法被拆出一部分进行共享。

受到操作系统使用带分页的虚拟内存解决了内存碎片和共享的方案启发，PagedAttention 将请求的 KV Cache 划分成固定大小的块（blocks），每个 block 存储固定数量 tokens 对应的 KV Cache 数据。在 PagedAttention 中，KV Cache 的 blocks 不一定存储在连续的空间中。因此，我们可以像操作系统的虚拟内存一样以更灵活的方式管理 KV Cache：将 block 看作页，将 token 看作字节，将 sequence 看作进程。这种设计通过使用相对较小的块并按需分配它们来减轻内部碎片。此外，它消除了外部碎片，因为所有块都具有相同的大小。最后，它支持以 block 为粒度，跨同一请求关联的不同序列甚至跨不同请求的内存共享。

#### continuous batching

由于 LLM 巨大的 GPU 内存开销和计算成本，在大多数应用中，机器学习工程师通常通过内部调整（如量化和对 CUDA 核的定制）来优化。然而，由于 LLM 通过迭代生成其输出，并且 LLM 推理通常涉及内存而不是计算，因此在很多实践中，优化系统级批处理可以使性能差异达到10倍甚至更多。

一种最近提出的优化方法是连续批处理（Continuous batching），也称为动态批处理或基于迭代级的批处理。其具有如下惊人的效果：

基于vLLM，使用连续批处理和连续批处理特定的内存优化，可以实现多达23倍的吞吐量提升；

对于 HuggingFace 本地生成推理，使用连续批处理，可以实现8倍的吞吐量提升；

基于 NVIDIA 的 FasterTransformer，使用优化过的模型实现，可以实现4倍的吞吐量提升。

## 推理优化算法介绍

#### prefill多batch推理

开发完成-增加推理服务吞吐量，在llmserving的基础上，原先只支持decoding的多batch推理，现在完善了多batch的prefill阶段的推理，大幅提升推理效率。

#### **warmup预热推理引擎**

通过对于mindformer的预先调用，减少第一次调用的额外耗时，同时将输入作为多batch进行处理确保输出多条请求结果。

```
    def warmup_and_multiPrefill(self,cfg:ServingConfig):
        batchsize = cfg['model_config']['prefill_batch_size'][0]
        seq_len = cfg['model_config']['seq_length'][0]
        input_ids = np.array([
        [1, 2, 3, 4, 5, 6, 7, 8] + [0] * (seq_len - 8)
        ] * batchsize, dtype=np.int64)
        valid_length_each_example = np.array([8] * batchsize, dtype=np.int64)
        current_index = np.array([7] * batchsize, dtype=np.int64)
        block_tables = np.array([
        [0] + [-1] * (256 - 1)
        for i in range(batchsize)
        ], dtype=np.int32)
        slot_mapping = np.array(list(range(10)) + [-1] * (seq_len*batchsize - 10), dtype=np.int32)
        model_kwargs = {"current_index": current_index}
        self.mindspore_model.is_first_iteration = True
        # self.mindspore_model.config.batch_size=4
        res, current_index = self.mindspore_model.forward(input_ids=input_ids,
                                                valid_length_each_example=valid_length_each_example,
                                                generation_config=self.mindspore_model.config,
                                                block_tables=block_tables,
                                                slot_mapping=slot_mapping,
                                                prefill=True,
                                                **model_kwargs)
```

#### 基于贪婪搜索完成端到端后处理

将模型的logits输出，直接进行argmax计算，而不是将logits传出，然后再传入进行计算，大大增加了效率。

```
修改/home/ma-user/work/mindformers/mindformers/models/llama/llama.py 补充贪婪搜索策略逻辑
        logits = self.cast(logits, mstype.float32)
        indices, max_values = self.argmax_with_value(logits)
        indices = P.Cast()(indices, mstype.int32)
        if not self.training:
            return indices, max_values
```

得到的结果就是词表的索引和对应的值

所以可以直接将结果写入到共享内存中，而不需要再次进行计算。

```python
    def handle_inference_results(self,outputs, outputs_shm, output_logprob_shm, decode_index, prefill=True):
        # self.targets.clear()
        indices=outputs[0]
        max_values=outputs[1]
        # 记录索引和对应的最大值日志
        logging.info("handle_inference_results indices type is {}, value is {}".format(indices.dtype, indices))
        logging.info("handle_inference_results max_values type is {}, value is {}".format(max_values.dtype, max_values))
        indices=indices.asnumpy().reshape(-1)
        max_values=max_values.asnumpy().reshape(-1)
        # 当 rank_id 为 0 时，将数据写入共享内存
        if self.rank_id == 0:
            if prefill:
                for index in decode_index:
                    tmp = np.ndarray((index + self.current_batch_size,), dtype=indices.dtype, buffer=outputs_shm.buf)
                    tmp[index: index + self.current_batch_size] = indices[:]


                    tmp_logprob = np.ndarray((index + self.current_batch_size,), dtype=np.float64,
                                             buffer=output_logprob_shm.buf)
                    tmp_logprob[index: index + self.current_batch_size] = max_values[:]

            else:
                # 非预填充模式
                tmp = np.ndarray((self.current_batch_size,), dtype=indices.dtype, buffer=outputs_shm.buf)
                tmp[:] = indices[:]

                # logprob_list = []
                # for idx, tag in enumerate(indices):
                #     logprob_list.append(max_values[idx])


                # 将对应的最大值写入共享内存
                tmp_logprob = np.ndarray((self.current_batch_size,), dtype=np.float64, buffer=output_logprob_shm.buf)
                tmp_logprob[:] = max_values[:]
                
                # 更新 targets
                # self.targets[:] = indices[:]
```



#### **==任务调度策略==**

本次比赛中，推理请求的输出token个数是已知的，那么就可以根据输出请求的token数量大小，给这些推理任务设置不同的优先级。传统的任务策略是FCFS策略，就是说谁先到，先服务谁。

既然已经知道推理的token数，那么我们就可以优先完成那些token生成数比较多的请求的prefill阶段，那么在后续batchsize为128的decoding请求中，有效的槽位才会更多，NPU的利用率也就非常大。

在4096句子长度下，分别快40秒和65秒.

#### ==fix bug==

```python
    /home/ma-user/work/llm-serving/mindspore_serving/server/llm_server_post.py
    async def _master_abort(self, request_ids):
        logging.debug(f"Master abort called with request_ids type: {type(request_ids)}, value: {request_ids}")
        # self.master.abort_request(request_ids)
        for request_id in request_ids:
            self.master.abort_request(request_id)
```

请求完成后，将传递给scheduler中的进行处理，设置为停止状态

```
    def abort_entry(self,
                    request_id: str):
        for index, data in enumerate(self.running_request_list):
            if data.request_id == request_id:
                self.running_request_list[index].get_entry_data().set_status(EntryStatus.FINISHED_STOPPED)
```

但是传递进来的是一个set对象，需要循环去调用。

没有及时将这些请求设置为完成状态，下一次调度推理就仍然会再次进行，那么就会导致已经推理完成的请求多推理一次，直到下一次超过生成token的限制时，才会被移除running队列。





#### ==一些数据类型转换上的优化==

程序中有很多数据类型上的转换。

例如

推理过程中self.kbk_targets = np.full((decode_batch_size, seq_length), self.config.model_config.pad_token_id)

初始化完是int64,应该是int32类型

input_ids第一次输入是int32类型的

在打平的时候，把数据格式由int32转为float64了，代码应该这么写

```python
input_ids = np.concatenate((input_ids, np.zeros((input_ids.shape[0], seq_length - 1))), axis=1)


input_ids = np.concatenate((input_ids, np.zeros((input_ids.shape[0], seq_length - 1), dtype=np.int32)), axis=1)

```

类似问题很多，这些问题都会导致推理过程中的一些性能损耗。



#### 针对qkv权重进行拼接，合并Gemm运算

对qkv的weight进行合并以后，只需要调用一次Gemm运算，就可以完成三个矩阵的运算，减少了kernel launch的时间

```python
        if self.qkv_concat:
            self.w = Linear(in_channels=self.hidden_size,
                            out_channels=self.hidden_size + self.kv_dim * 2,
                            has_bias=qkv_has_bias,
                            compute_dtype=compute_dtype,
                            param_init_type=param_init_type,
                            skip_redistribution=is_dynamic)
            self.w.shard(((dp, 1), (mp, 1)))
```



#### 调整seqlen大小，以提升mindformer的推理速递。

根据不同batch情况，调整seq_len大小，不同情况使用不同句子长度。

## 超参配置介绍

测速脚本配置：

```shell
python test_serving_performance.py -X 150 -P 8835 -O "./" -T 10
```



每秒发送150包，一共发送10秒。共计1500条推理请求。



llm-serving/configs/llama/llama_7b_kbk_pa_dyn.yaml

```yaml
model_config:
    model_name: 'llama_7b'
    max_generate_length: 4096
    end_token: 2
    seq_length: [4096]
    vocab_size: 32000
    prefill_batch_size: [8]
    decode_batch_size: [128]
    zactivate_len: [512, 1024, 2048, 4096]
    model_type: 'dyn'
    seq_type: 'static'
    batch_waiting_time: 0.0
    decode_batch_waiting_time: 0.0
    batching_strategy: 'continuous'
    current_index: False
    page_attention: True
    model_dtype: "DataType.FLOAT32"
    pad_token_id: 0
    backend: 'kbk' # 'ge'
    model_cfg_path: '/home/ma-user/work/mindformers/configs/llama2/predict_llama2_7b.yaml'

serving_config:
    agent_ports: [16002]
    start_device_id: 0
    server_ip: '127.0.0.1'
    server_port: 8835

pa_config:
    num_blocks: 1024
    block_size: 16
    decode_seq_length: 4096

tokenizer:
    type: LlamaTokenizer
    vocab_file: '/home/ma-user/work/checkpoint_download/llama2/tokenizer.model'

basic_inputs:
    type: LlamaBasicInputs

extra_inputs:
    type: LlamaExtraInputs

warmup_inputs:
    type: LlamaWarmupInputs
```



### 完整源码包

https://tuilizhuany.obs.cn-southwest-2.myhuaweicloud.com/happycode.zip



